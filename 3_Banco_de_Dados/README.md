# Teste 3: Banco de Dados e AnÃ¡lise SQL

Este mÃ³dulo Ã© responsÃ¡vel pela estruturaÃ§Ã£o do armazenamento de dados, implementaÃ§Ã£o de pipelines de carga (ETL) via SQL e desenvolvimento de consultas analÃ­ticas para responder a perguntas estratÃ©gicas de negÃ³cio.

## ğŸ“‹ Funcionalidades Implementadas

1.  **Modelagem de Dados:** DefiniÃ§Ã£o de esquema relacional (DDL) otimizado para integridade transacional e performance de leitura.
2.  **ETL via SQL:** Scripts de importaÃ§Ã£o (`LOAD DATA`) que tratam inconsistÃªncias de formataÃ§Ã£o (datas, pontuaÃ§Ã£o decimal e encoding) durante a carga.
3.  **AnÃ¡lise de NegÃ³cio:** Queries complexas utilizando *Window Functions* e *CTEs* para mÃ©tricas de crescimento e distribuiÃ§Ã£o geogrÃ¡fica.

---

## âš–ï¸ DecisÃµes TÃ©cnicas e Trade-offs (AnÃ¡lise CrÃ­tica)

Conforme solicitado no desafio, abaixo estÃ£o as justificativas para as escolhas de arquitetura e implementaÃ§Ã£o no banco de dados.

### 1. Modelagem: NormalizaÃ§Ã£o vs DesnormalizaÃ§Ã£o
**CenÃ¡rio:** Armazenar dados cadastrais de operadoras e milhÃµes de registros de despesas.
* **EstratÃ©gia Escolhida:** Modelo HÃ­brido (*Snowflake Schema* simplificado).
* **Justificativa:**
    * **NormalizaÃ§Ã£o (Tabelas `fato_despesas` e `dim_operadoras`):** Separei os dados cadastrais (RazÃ£o Social, UF) dos financeiros. Como a RazÃ£o Social se repete milhÃµes de vezes, normalizar economiza espaÃ§o e facilita atualizaÃ§Ãµes cadastrais (3FN).
    * **DesnormalizaÃ§Ã£o (Tabela `agg_despesas_uf`):** Mantive a tabela agregada (gerada no Teste 2) separada. Isso evita recÃ¡lculos custosos de `SUM/AVG` em relatÃ³rios de acesso frequente, priorizando a velocidade de leitura.

### 2. Tipos de Dados (PrecisÃ£o e Performance)
**CenÃ¡rio:** Definir tipos para valores monetÃ¡rios e datas.
* **EstratÃ©gia Escolhida:** `DECIMAL(18,2)` e `DATE`.
* **Justificativa:**
    * **Dinheiro:** Preterido o uso de `FLOAT` ou `DOUBLE` para evitar erros de arredondamento binÃ¡rio. O `DECIMAL` garante a precisÃ£o exata dos centavos exigida em auditorias financeiras.
    * **Datas:** Preterido o uso de `VARCHAR`. O tipo `DATE` ocupa menos espaÃ§o (3 bytes) e permite indexaÃ§Ã£o temporal e funÃ§Ãµes nativas (`DATEDIFF`, `YEAR`), essenciais para a performance da Query 1.

### 3. EstratÃ©gia de ImportaÃ§Ã£o (ETL)
**CenÃ¡rio:** Arquivos CSV com formataÃ§Ã£o brasileira (DD/MM/AAAA, vÃ­rgula decimal) incompatÃ­vel com o padrÃ£o SQL.
* **EstratÃ©gia Escolhida:** `LOAD DATA LOCAL INFILE` com variÃ¡veis de sessÃ£o (`@variavel`).
* **Justificativa:**
    * **Performance:** A inserÃ§Ã£o em lote (*bulk insert*) Ã© ordens de magnitude mais rÃ¡pida que `INSERT` linha a linha.
    * **SanitizaÃ§Ã£o:** Tratamos as inconsistÃªncias (conversÃ£o de data e troca de vÃ­rgula por ponto) *on-the-fly* durante a carga, garantindo que o dado sÃ³ entre no banco se estiver limpo.

### 4. LÃ³gica de Queries AnalÃ­ticas
**CenÃ¡rio:** Calcular crescimento percentual ignorando trimestres sem dados ("buracos" no histÃ³rico).
* **EstratÃ©gia Escolhida:** *Window Functions* (`FIRST_VALUE` ordenado por data).
* **Justificativa:**
    * **Robustez:** Identificamos dinamicamente o primeiro e o Ãºltimo registro real de cada operadora em uma Ãºnica passagem (scan), sem a necessidade de *Self-Joins* complexos e lentos.

---

## ğŸš€ Como Executar

Este mÃ³dulo requer um servidor **MySQL 8.0+** e depende dos arquivos gerados nos Testes 1 e 2.

1.  **PrÃ©-requisitos:**
    * Habilite a carga de arquivos locais no seu cliente MySQL: `local_infile=1`.
    * Certifique-se de que os Testes 1 e 2 foram executados (os CSVs sÃ£o dependÃªncias).

2.  **Execute os scripts na ordem:**
    ```sql
    source 1_create_tables.sql;      -- Cria a estrutura
    source 2_import_data.sql;        -- Carrega e limpa os dados
    source 3_queries_analiticas.sql; -- Executa os relatÃ³rios
    ```

3.  **Resultado:**
    O console exibirÃ¡ o status da importaÃ§Ã£o e os resultados tabulares das 3 queries solicitadas.

---

## ğŸ“‚ Estrutura do MÃ³dulo

Os scripts SQL foram numerados sequencialmente para garantir a ordem lÃ³gica de execuÃ§Ã£o (DDL -> DML -> DQL).

```text
3_Banco_de_Dados/
â”‚
â”œâ”€â”€ 1_create_tables.sql      # DDL: DefiniÃ§Ã£o do Schema, Tabelas e Ãndices
â”œâ”€â”€ 2_import_data.sql        # DML: Script de Carga e Tratamento de Dados (ETL)
â”œâ”€â”€ 3_queries_analiticas.sql # DQL: Consultas AnalÃ­ticas (Respostas do Teste)
â””â”€â”€ README.md                # DocumentaÃ§Ã£o tÃ©cnica e justificativas de arquitetura
```

## ğŸ‘¤ Autor: Ãtallo de Santana GuimarÃ£es